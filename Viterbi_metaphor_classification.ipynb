{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pos_seq</th>\n",
       "      <th>label_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ca n't fail to be entertaining .</td>\n",
       "      <td>['VERB', 'ADV', 'VERB', 'PART', 'VERB', 'ADJ',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much was he going to tell her ?</td>\n",
       "      <td>['ADV', 'ADJ', 'VERB', 'PRON', 'VERB', 'PART',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Up until that news hit the Committee , Don had...</td>\n",
       "      <td>['ADP', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', '...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Could go on to the rugby and go with them coul...</td>\n",
       "      <td>['VERB', 'VERB', 'PART', 'ADP', 'DET', 'NOUN',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finally , we went to the office and they gave ...</td>\n",
       "      <td>['ADV', 'PUNCT', 'PRON', 'VERB', 'ADP', 'DET',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0                   Ca n't fail to be entertaining .   \n",
       "1                How much was he going to tell her ?   \n",
       "2  Up until that news hit the Committee , Don had...   \n",
       "3  Could go on to the rugby and go with them coul...   \n",
       "4  Finally , we went to the office and they gave ...   \n",
       "\n",
       "                                             pos_seq  \\\n",
       "0  ['VERB', 'ADV', 'VERB', 'PART', 'VERB', 'ADJ',...   \n",
       "1  ['ADV', 'ADJ', 'VERB', 'PRON', 'VERB', 'PART',...   \n",
       "2  ['ADP', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', '...   \n",
       "3  ['VERB', 'VERB', 'PART', 'ADP', 'DET', 'NOUN',...   \n",
       "4  ['ADV', 'PUNCT', 'PRON', 'VERB', 'ADP', 'DET',...   \n",
       "\n",
       "                                           label_seq  \n",
       "0                              [0, 0, 0, 0, 0, 0, 0]  \n",
       "1                        [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "rel_path = '/data_release'\n",
    "filename = '/train.csv'\n",
    "train_data = pd.read_csv(cwd + rel_path + filename, encoding = \"ISO-8859-1\")\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown Word Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_words(wordlist):\n",
    "    for i in range(len(wordlist)):\n",
    "        toss = np.random.binomial(size=1, n=1, p= 0.005)\n",
    "        if toss == 1:\n",
    "            wordlist[i] = 'UNK'\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all sentences into corresponding Words in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116622\n"
     ]
    }
   ],
   "source": [
    "# O(n) Complexity\n",
    "\n",
    "sentence = train_data['sentence']\n",
    "sentence = list(sentence)\n",
    "word_str = ''\n",
    "for item in sentence:\n",
    "    word_str += item + ' '\n",
    "\n",
    "word_str = word_str.lower()\n",
    "word_list = word_str.split()\n",
    "word_list = unknown_words(word_list)          # def unknown_words()\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all metaphor labels into one huge list of labels of 0's and 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116622"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O(n) complexity\n",
    "\n",
    "tag = train_data['label_seq'].tolist()\n",
    "tag_str = ''\n",
    "for item in tag:\n",
    "    item = item[1:-1]\n",
    "    tag_str += item + ', '\n",
    "    \n",
    "tag_list_str = tag_str.split(', ')\n",
    "tag_list_str = tag_list_str[0:-1]\n",
    "tag_list = []\n",
    "for item in tag_list_str:\n",
    "    tag_list.append(int(item))\n",
    "\n",
    "len(tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary for each word and its count of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[499, 71]\n"
     ]
    }
   ],
   "source": [
    "# O(n) Complexity\n",
    "\n",
    "word_dict = {}\n",
    "\n",
    "for item, tag in zip(word_list, tag_list):\n",
    "    if item not in word_dict:\n",
    "        val_list = [0,0]\n",
    "        val_list[tag] += 1\n",
    "        word_dict[item] = val_list\n",
    "    else:\n",
    "        val_list = word_dict[item]\n",
    "        val_list[tag] += 1\n",
    "        word_dict[item] = val_list\n",
    "        \n",
    "print(word_dict['UNK'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary of Tag counts (Unigrams and Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 103571, '<s>': 6323, '1': 13051, '00': 87135, '0<s>': 6267, '<s>0': 5829, '01': 10169, '10': 10606, '11': 2389, '<s>1': 493, '1<s>': 56}\n"
     ]
    }
   ],
   "source": [
    "# O(n) Complexity\n",
    "\n",
    "tag_dict = {}\n",
    "\n",
    "tag = train_data['label_seq'].tolist()\n",
    "tag_str = ''\n",
    "for item in tag:\n",
    "    item = item[1:-1]\n",
    "    tag_str += item + ', ' + '<s>' + ', '\n",
    "\n",
    "tag_list_str_s = tag_str.split(', ')\n",
    "tag_list_str_s = tag_list_str_s[0:-1]\n",
    "\n",
    "tag_list_str_bigram = []\n",
    "for i in range(len(tag_list_str_s)-1):\n",
    "    tag_list_str_bigram.append(tag_list_str_s[i]+tag_list_str_s[i+1])\n",
    "\n",
    "for item in tag_list_str_s:\n",
    "    if item not in tag_dict:\n",
    "        tag_dict[item] = 1\n",
    "    else:\n",
    "        tag_dict[item] += 1\n",
    "\n",
    "for item in tag_list_str_bigram:\n",
    "    if item not in tag_dict:\n",
    "        tag_dict[item] = 1\n",
    "    else:\n",
    "        tag_dict[item] += 1\n",
    "\n",
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transitional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_prob(tag_dict, i, j):\n",
    "    if j==-1:\n",
    "        prob = tag_dict[str(i)]/(tag_dict[str(0)] + tag_dict[str(1)])\n",
    "    else:\n",
    "        key = str(j) + str(i)\n",
    "        prob = tag_dict[key]/tag_dict[str(j)]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emiss_prob(word_dict,tag_dict, word, i):\n",
    "    \n",
    "    if word in word_dict:\n",
    "        prob = word_dict[word][i]/tag_dict[str(i)]\n",
    "    else:\n",
    "        prob = word_dict['UNK'][i]/tag_dict[str(i)]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS with ID tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'VERB'\": 1, \"'ADV'\": 2, \"'PART'\": 3, \"'ADJ'\": 4, \"'PUNCT'\": 5, \"'PRON'\": 6, \"'ADP'\": 7, \"'DET'\": 8, \"'NOUN'\": 9, \"'PROPN'\": 10, \"'CCONJ'\": 11, \"'NUM'\": 12, \"'INTJ'\": 13, \"'X'\": 14, \"'SYM'\": 15}\n"
     ]
    }
   ],
   "source": [
    "pos_seq = train_data['pos_seq'].tolist()\n",
    "pos_str = ''\n",
    "for item in pos_seq:\n",
    "    item = item[1:-1]\n",
    "    pos_str += item + ', '\n",
    "\n",
    "pos_list = pos_str.split(', ')\n",
    "pos_list = pos_list[0:-1]\n",
    "pos_dict = {}\n",
    "\n",
    "i = 1\n",
    "for item in pos_list:\n",
    "    if item not in pos_dict:\n",
    "        pos_dict[item] = i\n",
    "        i += 1\n",
    "\n",
    "print(pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Feature Matrix for Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_feature_matrix(feature_len, data, wordlist):    \n",
    "#     feature_len = 9           # posi-4, posi-3, posi-2, posi-1, posi, posi+1, posi+2, posi+3, posi+4 \n",
    "    Lend = int((feature_len-1)/2)\n",
    "    nrow = len(wordlist)\n",
    "    ncol = feature_len\n",
    "    # feature_train_X = np.full((nrow, ncol), -1, dtype=int)\n",
    "    feature_X = np.zeros((nrow, ncol), dtype=int)\n",
    "\n",
    "    sentence = data['sentence'].tolist()\n",
    "    tagseq = data['pos_seq'].tolist()\n",
    "    \n",
    "    # sentence = sentence[0:1]\n",
    "    word_c = 0\n",
    "    for item,tag in zip(sentence, tagseq):\n",
    "        item = item.split()\n",
    "        tag = tag[1:-1]\n",
    "        tag = tag.split(', ')\n",
    "\n",
    "        for i in range(len(item)):\n",
    "            for j in range(-Lend, Lend+1):\n",
    "                if(i+j>=0 and i+j<len(item)):\n",
    "                    feature_X[word_c, (j+Lend)] = pos_dict[tag[i+j]]\n",
    "            word_c=word_c+1 \n",
    "\n",
    "    return feature_X      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is an alternative implementation of Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def develop_feature_matrix(feature_len, data, wordlist):    \n",
    "# #     feature_len = 9           # posi-4, posi-3, posi-2, posi-1, posi, posi+1, posi+2, posi+3, posi+4 \n",
    "#     Lend = int((feature_len-1)/2)\n",
    "#     nrow = len(wordlist)\n",
    "#     ncol = feature_len\n",
    "#     # feature_train_X = np.full((nrow, ncol), -1, dtype=int)\n",
    "#     feature_X = np.zeros((nrow, (ncol+1)), dtype=int)\n",
    "    \n",
    "#     sentence = data['sentence'].tolist()\n",
    "#     tagseq = data['pos_seq'].tolist()\n",
    "    \n",
    "#     # sentence = sentence[0:1]\n",
    "#     word_c = 0\n",
    "#     for item,tag in zip(sentence, tagseq):\n",
    "#         item = item.split()\n",
    "#         tag = tag[1:-1]\n",
    "#         tag = tag.split(', ')\n",
    "\n",
    "#         for i in range(len(item)):\n",
    "#             for j in range(-Lend, Lend+1):\n",
    "#                 if(i+j>=0 and i+j<len(item)):\n",
    "#                     feature_X[word_c, (j+Lend)] = pos_dict[tag[i+j]]\n",
    "            \n",
    "#             feature_X[word_c, (ncol)] = word_dict[word_c][1]\n",
    "#             word_c=word_c+1 \n",
    "\n",
    "#     return feature_X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_len = 9\n",
    "# X_train = develop_feature_matrix(feature_len, train_data, word_list)\n",
    "# X_train[0:11,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(feature_X, label_Y):    \n",
    "    clf = MultinomialNB(alpha = 1, class_prior=None, fit_prior=True)\n",
    "    clf.fit(feature_X, label_Y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_prob_naive_bayes(model, X):\n",
    "    Ypred = model.predict_proba(X)\n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(feature_X, label_Y):    \n",
    "    weight = {0:1,1:4}\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs', class_weight = weight, multi_class='multinomial')\n",
    "    clf.fit(feature_X, label_Y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_logistic_regression(model, X):\n",
    "    Ypred = model.predict_proba(X)\n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm on Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(item, Y_prob, word_dict, tag_dict, ntags, word_c):\n",
    "    \n",
    "    item = item.split()\n",
    "    word0 = item[0]\n",
    "    c_prev = -1\n",
    "    score = np.zeros((ntags, len(item)))\n",
    "    b_ptr = np.zeros((ntags, len(item)))\n",
    "    \n",
    "    for c in range(ntags):\n",
    "#         score[c,0] = math.log(t_prob(tag_dict, c, c_prev)) + math.log(emiss_prob(word_dict, tag_dict, word0, c))\n",
    "#         score[c,0] = t_prob(tag_dict, c, c_prev)*emiss_prob(word_dict, tag_dict, word0, c)\n",
    "#         score[c,0] = t_prob(tag_dict, c, c_prev)*Y_prob[0,c]\n",
    "        score[c,0] = Y_prob[0,c]*emiss_prob(word_dict, tag_dict, word0, c)\n",
    "        b_ptr[c,0] = 0\n",
    "    \n",
    "    word_c += 1\n",
    "    for t in range(1, len(item)):\n",
    "        for c in range(ntags):\n",
    "            emission_prob = emiss_prob(word_dict, tag_dict, item[t], c)\n",
    "            temp = []\n",
    "            for j in range(ntags):\n",
    "#                 temp.append(score[j,t-1] + math.log(t_prob(tag_dict, c, j)) + log(emission_prob))\n",
    "#                 temp.append(score[j,t-1]*t_prob(tag_dict, c, j)*emission_prob)\n",
    "#                 temp.append(score[j,t-1]*t_prob(tag_dict, c, j)*Y_prob[word_c,c])\n",
    "                temp.append(score[j,t-1]*Y_prob[word_c,c]*emission_prob)\n",
    "            score[c,t] = max(temp)\n",
    "            b_ptr[c,t] = int(temp.index(max(temp)))\n",
    "        word_c += 1\n",
    "    \n",
    "    b_ptr = np.int_(b_ptr)\n",
    "    Tseq = [-1]*len(item)\n",
    "    Tseq[len(item)-1] = np.argmax(score[:,len(item)-1])\n",
    "    for i in range(len(item)-2, -1, -1):\n",
    "        Tseq[i] = b_ptr[Tseq[i+1], i+1]\n",
    "    \n",
    "    return Tseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model on a *classifier* and Predicting on Validation/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pos_seq</th>\n",
       "      <th>label_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Four alternative approaches have been describe...</td>\n",
       "      <td>['NUM', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB',...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I wanted to say , you see , that I know you th...</td>\n",
       "      <td>['PRON', 'VERB', 'PART', 'VERB', 'PUNCT', 'PRO...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The one with you chop the chop and then there ...</td>\n",
       "      <td>['DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'DET', ...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given that most GIS are rather dumb systems , ...</td>\n",
       "      <td>['VERB', 'ADP', 'ADJ', 'PROPN', 'VERB', 'ADV',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lacking a goal that might have altered its che...</td>\n",
       "      <td>['VERB', 'DET', 'NOUN', 'ADJ', 'VERB', 'VERB',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  Four alternative approaches have been describe...   \n",
       "1  I wanted to say , you see , that I know you th...   \n",
       "2  The one with you chop the chop and then there ...   \n",
       "3  Given that most GIS are rather dumb systems , ...   \n",
       "4  Lacking a goal that might have altered its che...   \n",
       "\n",
       "                                             pos_seq  \\\n",
       "0  ['NUM', 'ADJ', 'NOUN', 'VERB', 'VERB', 'VERB',...   \n",
       "1  ['PRON', 'VERB', 'PART', 'VERB', 'PUNCT', 'PRO...   \n",
       "2  ['DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'DET', ...   \n",
       "3  ['VERB', 'ADP', 'ADJ', 'PROPN', 'VERB', 'ADV',...   \n",
       "4  ['VERB', 'DET', 'NOUN', 'ADJ', 'VERB', 'VERB',...   \n",
       "\n",
       "                                           label_seq  \n",
       "0         [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2            [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Validation Set\n",
    "cwd = os.getcwd()\n",
    "rel_path = '/data_release'\n",
    "filename = '/val.csv'\n",
    "# filename = '/test_no_label.csv'\n",
    "val_data = pd.read_csv(cwd + rel_path + filename, encoding = \"ISO-8859-1\")\n",
    "val_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38628\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label\n",
       "idx       \n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "5        0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trained model on training data\n",
    "feature_len = 5\n",
    "X_train = develop_feature_matrix(feature_len, train_data, word_list)      # feature_matrix for classifier\n",
    "Y_truth = tag_list\n",
    "# model = naive_bayes(X_train, Y_truth)                                   # Classifier\n",
    "model = logistic_regression(X_train, Y_truth)                             # Classifier\n",
    "\n",
    "\n",
    "# O(n) Complexity\n",
    "\n",
    "sentence = val_data['sentence']\n",
    "sentence = list(sentence)\n",
    "word_str = ''\n",
    "for item in sentence:\n",
    "    word_str += item + ' '\n",
    "\n",
    "word_str = word_str.lower()\n",
    "val_word_list = word_str.split()\n",
    "\n",
    "for i in range(len(val_word_list)):\n",
    "    if val_word_list[i] not in word_list:\n",
    "        val_word_list[i] = 'UNK'\n",
    "\n",
    "X_val =  develop_feature_matrix(feature_len, val_data, val_word_list)     # feature_matrix for classifier\n",
    "# Y_prob = classifier_prob_naive_bayes(model, X_val)                      # Prediction from classifier\n",
    "Y_prob = classifier_prob_naive_bayes(model, X_val)                        # Prediction from classifier\n",
    "\n",
    "ntags = 2\n",
    "tagseq = []                                   # Predicted Labels\n",
    "word_c = 0\n",
    "for item in sentence:\n",
    "    item = item.lower()\n",
    "#     tagseq += viterbi(item, word_dict, tag_dict, ntags)\n",
    "    tagseq += viterbi(item, Y_prob, word_dict, tag_dict, ntags, word_c)\n",
    "    word_c += len(item.split())\n",
    "\n",
    "print(len(tagseq))\n",
    "\n",
    "output = pd.DataFrame(tagseq)\n",
    "output.columns = ['label']\n",
    "output.index.name = 'idx'\n",
    "output.index += 1\n",
    "path = os.getcwd() + '/result_classifier_val.csv'\n",
    "output.to_csv(path)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
